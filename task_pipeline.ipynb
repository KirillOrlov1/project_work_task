{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"task_pipeline.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Task pipeline"],"metadata":{"id":"QZL1csCzWg6P"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2fCGX_7pBdWM"},"outputs":[],"source":["import librosa\n","import pandas as pd\n","import numpy as np\n","import os\n","from PIL import Image\n","import copy\n","\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import torch\n","import torchaudio\n","import torchvision\n","\n","from sklearn.metrics import accuracy_score\n","\n","from torchvision.models import resnet34\n","\n","import warnings\n","warnings.filterwarnings(action='ignore', category=DeprecationWarning)"]},{"cell_type":"code","source":["SAMPLE_RATE = 48000\n","N_FFT = SAMPLE_RATE * 64 // 1000 + 4\n","HOP_LENGTH = SAMPLE_RATE * 16 // 1000 + 4"],"metadata":{"id":"0rYvxyGke9I4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class NoisedDataset(Dataset):\n","\n","    def __init__(self, csv_file, root_dir, n_fft, hop_length):\n","        \n","        self.csv_file = csv_file\n","        self.root_dir = root_dir\n","\n","        self.n_fft = n_fft\n","        self.hop_length = hop_length\n","\n","        self.max_len = 165000\n","    \n","    def __getitem__(self, idx):\n","\n","        ts1 = torchvision.transforms.Resize((299, 299))\n","        ts2 = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        \n","        sample = self.csv_file.values[idx]\n","        flag = 'noise' if sample[0].startswith('noised') else 'no_noise'\n","        path = self.root_dir + '/' + flag + '/' + sample[0]\n","\n","        signal, _ = torchaudio.load(path)\n","        signal = self._prepare_sample(signal)\n","\n","        spec = torch.stft(\n","            input=signal,\n","            n_fft=self.n_fft,\n","            hop_length=self.hop_length,\n","            normalized=True\n","        )\n","\n","        real = spec[..., 0]\n","        img = spec[..., 1]\n","        spec = torch.cat([real, img], dim=0)\n","        \n","        return [spec, sample[1]]\n","\n","    \n","    def __len__(self):\n","\n","        return self.csv_file.shape[0]\n","    \n","    def _prepare_sample(self, waveform):\n","        waveform = waveform.numpy()\n","        current_len = waveform.shape[1]\n","        \n","        output = np.zeros((1, self.max_len), dtype='float32')\n","        output[0, -current_len:] = waveform[0, :self.max_len]\n","        output = torch.from_numpy(output)\n","        \n","        return output"],"metadata":{"id":"desjoGf9I_Tj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_loop(model, train_data, test_data, EPOCH, batch_size):\n","\n","    load_train = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","    load_test = DataLoader(test_data, batch_size=batch_size)\n","\n","    history_loss = []\n","    history_acc = []\n","\n","    DEVICE = torch.device('cuda')\n","    criterion = torch.nn.MSELoss()\n","    optim = torch.optim.Adam(model.parameters())\n","\n","    for epoch in range(EPOCH):\n","\n","        ep_tr_loss = []\n","        ep_tr_acc = []\n","\n","        ep_test_loss = []\n","        ep_test_acc = []\n","\n","        model.cuda().train()\n","        for bx, by in load_train:\n","\n","            bx = bx.cuda()\n","            by = by.float()\n","\n","            optim.zero_grad()\n","            preds = model(bx)\n","            loss = criterion(preds.cpu(), by.unsqueeze(1))\n","            loss.backward()\n","            optim.step()\n","\n","            ep_tr_loss.append(loss.item())\n","            preds = np.around(preds.detach().cpu().numpy())\n","            by = np.reshape(by.numpy(), (-1, 1))\n","            ep_tr_acc.append(accuracy_score(by, preds))\n","        \n","        model.eval()\n","        for bx, by in load_test:\n","\n","            bx = bx.cuda()\n","            by = by.float()\n","\n","            with torch.no_grad():\n","                preds = model(bx)\n","            \n","            preds = preds.cpu()\n","            loss = criterion(preds, by.unsqueeze(1))\n","\n","            ep_test_loss.append(loss.item())\n","            preds = np.around(preds)\n","            by = np.reshape(by.numpy(), (-1, 1))\n","            ep_test_acc.append(accuracy_score(by, preds))\n","        \n","        loss1 = round(sum(ep_tr_loss) / len(ep_tr_loss), 4)\n","        acc1 = round(sum(ep_tr_acc) / len(ep_tr_acc), 4)\n","        loss2 = round(sum(ep_test_loss) / len(ep_test_loss), 4)\n","        acc2 = round(sum(ep_test_acc) / len(ep_test_acc), 4)\n","        print(f'epoch {epoch}: train loss {loss1}, train acc {acc1}, test loss {loss2}, test acc {acc2}')\n","\n","        history_loss.extend(ep_test_loss)\n","        history_acc.extend(ep_test_acc)\n","\n","        if EPOCH % 10 == 0:\n","            model_weigths = copy.deepcopy(model.state_dict())\n","            torch.save(model_weigths, f'/content/drive/MyDrive/weigths_{EPOCH}')\n","    return history_loss, history_acc"],"metadata":{"id":"kZtE0V6ZLWvS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Test models"],"metadata":{"id":"tPIGyQlXWoII"}},{"cell_type":"code","source":["! unzip /content/drive/MyDrive/background_noise_dataset.zip -d /content/background_noise_dataset"],"metadata":{"id":"eqUhCNJ_XBLa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = '/content/background_noise_dataset/content/background_noise_dataset'\n","train_csv = pd.read_csv('/content/drive/MyDrive/train.csv')\n","test_csv = pd.read_csv('/content/drive/MyDrive/test.csv')\n","\n","train_dataset = NoisedDataset(train_csv, path, N_FFT, HOP_LENGTH)\n","test_dataset = NoisedDataset(test_csv, path, N_FFT, HOP_LENGTH)"],"metadata":{"id":"hn9eSRzxWWdG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = resnet34()\n","model.conv1 = torch.nn.Conv2d(2, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","model.fc = torch.nn.Linear(in_features=512, out_features=1, bias=True)"],"metadata":{"id":"kBnC31GVEuQv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hist, acc = train_loop(model, train_dataset, test_dataset, 10, 32)"],"metadata":{"id":"6x7DcMZnazhC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_weigths = copy.deepcopy(model.state_dict())\n","torch.save(model_weigths, '/content/drive/MyDrive/weigths_final.pth')"],"metadata":{"id":"jG1_7Uj4gAYc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"W1s0L7XYpIqp"},"execution_count":null,"outputs":[]}]}